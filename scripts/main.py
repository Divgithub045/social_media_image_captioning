# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vh8dtqHYXn07-fbibsBLKwuzfx0S2zJo
"""

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import warnings
warnings.filterwarnings('ignore')
import numpy as np
import pickle
import tensorflow as tf

# FIX: Add compatibility for old tokenizer files
import sys
import tensorflow.keras.preprocessing as keras_preprocessing
sys.modules['keras'] = tf.keras
sys.modules['keras.preprocessing'] = keras_preprocessing
sys.modules['keras.preprocessing.text'] = keras_preprocessing.text

from tensorflow.keras.applications.inception_v3 import InceptionV3
from tensorflow.keras.preprocessing.image import img_to_array, load_img
from tensorflow.keras.utils import pad_sequences
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, add
from tensorflow.keras.layers import Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from PIL import Image

print("Loading saved tokenizer and model configuration...")

#assets\model\best_model_weights_flickr30.h5
# File paths
TOKENIZER_PATH = "assets\\model\\tokenizer_flickr30.pkl"
CONFIG_PATH = "assets\\model\\model_config_flickr30.pkl"
WEIGHTS_PATH = "assets\\model\\best_model_weights_flickr30.h5"

# Load saved tokenizer - REQUIRED
print(f"Loading tokenizer from {TOKENIZER_PATH}...")
with open(TOKENIZER_PATH, "rb") as f:
    tokenizer = pickle.load(f)
print("âœ… Tokenizer loaded successfully")

# Load saved model configuration - REQUIRED
print(f"Loading model config from {CONFIG_PATH}...")
with open(CONFIG_PATH, "rb") as f:
    model_config = pickle.load(f)

vocab_size = model_config['vocab_size']
max_caption_length = model_config['max_caption_length']
cnn_output_dim = model_config['cnn_output_dim']

print(f"âœ… Model config loaded:")
print(f"   Vocabulary size: {vocab_size}")
print(f"   Max caption length: {max_caption_length}")
print(f"   CNN output dimension: {cnn_output_dim}")

print("GPUs Available:", tf.config.list_physical_devices('GPU'))

def preprocess_image(image_path):
    print(f"Preprocessing image: {image_path}")
    img = load_img(image_path, target_size=(299, 299))
    img = img_to_array(img)
    img = np.expand_dims(img, axis=0)
    img = tf.keras.applications.inception_v3.preprocess_input(img)
    return img

def extract_image_features(model, image_path):
    print("Extracting image features...")
    img = preprocess_image(image_path)
    features = model.predict(img, verbose=0)
    return features

# Loading the pre-trained InceptionV3 model
print("Loading InceptionV3 model...")
gpus = tf.config.list_physical_devices('GPU')
device = '/GPU:0' if gpus else '/CPU:0'

with tf.device(device):
    inception_v3_model = InceptionV3(weights='imagenet', input_shape=(299, 299, 3))
    inception_v3_model.layers.pop()
    inception_v3_model = Model(inputs=inception_v3_model.inputs, outputs=inception_v3_model.layers[-2].output)

print(f"âœ… InceptionV3 model loaded on {device}")

def build_model(vocab_size, max_caption_length, cnn_output_dim):
    print("Building caption model...")
    # Encoder Model
    input_image = Input(shape=(cnn_output_dim,), name='Features_Input')
    fe1 = BatchNormalization()(input_image)
    fe2 = Dense(512, activation='relu')(fe1)
    fe3 = Dropout(0.5)(fe2)
    fe4 = Dense(256, activation='relu')(fe3)
    fe5 = BatchNormalization()(fe4)

    # Decoder Model
    input_caption = Input(shape=(max_caption_length,), name='Sequence_Input')
    se1 = Embedding(vocab_size, 256, mask_zero=True)(input_caption)
    se2 = LSTM(256, return_sequences=True)(se1)
    se3 = LSTM(256)(se2)
    se4 = Dropout(0.5)(se3)

    # Output
    decoder1 = add([fe5, se4])
    decoder2 = Dense(256, activation='relu')(decoder1)
    decoder3 = Dropout(0.5)(decoder2)
    outputs = Dense(vocab_size, activation='softmax', name='Output_Layer')(decoder3)

    model = Model(inputs=[input_image, input_caption], outputs=outputs, name='Image_Captioning')
    return model

caption_model = build_model(vocab_size, max_caption_length, cnn_output_dim)

print("Compiling model...")
optimizer = Adam(learning_rate=0.01, clipnorm=1.0)
caption_model.compile(loss='categorical_crossentropy', optimizer=optimizer)

# Load pre-trained weights - REQUIRED
print(f"Loading pre-trained weights from {WEIGHTS_PATH}...")
caption_model.load_weights(WEIGHTS_PATH)
print("âœ… Model weights loaded successfully")

def greedy_generator(image_features):
    print("Generating caption using greedy search...")
    in_text = 'start'

    for _ in range(max_caption_length):
        sequence = tokenizer.texts_to_sequences([in_text])[0]
        sequence = pad_sequences([sequence], maxlen=max_caption_length).reshape((1, max_caption_length))

        prediction = caption_model.predict(
            [image_features.reshape(1, cnn_output_dim), sequence],
            verbose=0
        )
        idx = np.argmax(prediction)

        if idx == 0:   # skip <PAD>
            continue
        word = tokenizer.index_word.get(idx, "<UNK>")

        in_text += ' ' + word

        if word == 'end':
            break

    # Clean special tokens
    final_caption = in_text.replace('start ', '', 1).replace(' end', '')
    print(f"Generated caption: {final_caption}")
    return final_caption

def beam_search_generator(image_features, K_beams=3, log=False):
    print(f"Generating caption using beam search (K={K_beams})...")
    start = [tokenizer.word_index['start']]
    start_word = [[start, 0.0]]

    for _ in range(max_caption_length):
        temp = []
        for s in start_word:
            sequence = pad_sequences([s[0]], maxlen=max_caption_length).reshape((1, max_caption_length))

            preds = caption_model.predict(
                [image_features.reshape(1, cnn_output_dim), sequence],
                verbose=0
            )

            word_preds = np.argsort(preds[0])[-K_beams:]

            for w in word_preds:
                next_cap, prob = s[0][:], s[1]
                next_cap.append(w)
                if log:
                    prob += np.log(preds[0][w] + 1e-10)
                else:
                    prob += preds[0][w]
                temp.append([next_cap, prob])

        start_word = sorted(temp, key=lambda l: l[1])
        start_word = start_word[-K_beams:]

    best_seq = start_word[-1][0]

    captions_ = [
        tokenizer.index_word.get(int(i), "<UNK>")
        for i in best_seq
        if int(i) != 0
    ]

    final_caption = []
    for w in captions_:
        if w == 'end':
            break
        if w not in ['start', '<PAD>']:
            final_caption.append(w)

    result = ' '.join(final_caption)
    print(f"Generated caption: {result}")
    return result

def generate_caption(image_path, method='greedy', K_beams=3):
    print(f"Generating caption for: {image_path}")
    print(f"Method: {method}")
    
    image_features = extract_image_features(inception_v3_model, image_path)
    
    if method == 'greedy':
        caption = greedy_generator(image_features.flatten())
    elif method == 'beam_search':
        caption = beam_search_generator(image_features.flatten(), K_beams=K_beams)
    else:
        raise ValueError("Invalid method. Choose 'greedy' or 'beam_search'.")
    
    print(f"Final Generated Caption: {caption}")
    return caption

print("âœ… Caption generation system ready!")
print("ðŸ“ž Use generate_caption(image_path, method='greedy'/'beam_search', K_beams=3)")